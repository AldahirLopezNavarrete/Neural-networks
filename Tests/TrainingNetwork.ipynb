{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "825b8028",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4473f1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    \"\"\" A single neuron with the sigmoid activate function\n",
    "        Attributtes:\n",
    "            inputs: The number of inputs in the perceptron, not cunting the bias\n",
    "            bias: the bias term. By default itÂ´s 1.0\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, inputs, bias = 1.0):\n",
    "        \"\"\"Return a new Perceptron object with the specified number of inputs +1 (for the bias)\"\"\"\n",
    "        self.weights = np.random.rand(inputs+1)*2 -1\n",
    "        self.bias = bias\n",
    "        \n",
    "    def run(self,x):\n",
    "        \"\"\"Run the perceptron. x is a python list with the input values.\"\"\"\n",
    "        x_sum = np.dot(np.append(x,self.bias),self.weights)\n",
    "        #this calculates the product point of the inputs and the weights\n",
    "        return self.sigmoid(x_sum)\n",
    "    \n",
    "    def set_weights(self,w_init):\n",
    "        \"\"\"Set the weights. w_init is a python list with the weights\"\"\"\n",
    "        self.weights = np.array(w_init)\n",
    "\n",
    "    \n",
    "    def sigmoid(self,x):\n",
    "        \"\"\"Evaluate the sigmoid function for thw floating point input x\"\"\"\n",
    "        return 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef06d7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiLayerPerceptron:\n",
    "    \"\"\"A multilayer perceptron class that uses the perceptron class above.\n",
    "        Attributtes:\n",
    "            layers: A python list with the number of elements per layer\n",
    "            bias: The bias term. The same bias is used for all neurons\n",
    "            eta: The learning rate\"\"\"\n",
    "    def __init__(self,layers,bias = 1.0, eta = 0.5):\n",
    "        \"\"\"Return a new MLP object with the specified parameters\"\"\"\n",
    "        self.layers = np.array(layers,dtype=object)\n",
    "        self.bias = bias\n",
    "        self.eta = eta\n",
    "        self.network = [] #the list of all neurons\n",
    "        self.values = [] #the list of all output values\n",
    "        self.d= []\n",
    "        \n",
    "        for i in range(len(self.layers)):\n",
    "            self.values.append([])\n",
    "            self.d.append([])\n",
    "            self.network.append([])\n",
    "            self.values[i] = [0.0 for j in range(self.layers[i])]\n",
    "            self.d[i] = [0.0 for j in range(self.layers[i])]\n",
    "            if i>0:\n",
    "                for j in range(self.layers[i]):\n",
    "                    self.network[i].append(Perceptron(inputs=self.layers[i-1],bias=self.bias))\n",
    "    \n",
    "        self.network = np.array([np.array(x) for x in self.network],dtype=object)\n",
    "        self.values=np.array([np.array(x) for x in self.values],dtype=object)\n",
    "        self.d = np.array([np.array(x) for x in self.d],dtype=object)\n",
    "    \n",
    "    def set_weights(self,w_init):\n",
    "        \"\"\"set the weights.\n",
    "            w_init is a list of lists with the weights for all, but the input layer\"\"\"\n",
    "        for i in range(len(w_init)):\n",
    "            for j in range(len(w_init[i])):\n",
    "                self.network[i+1][j].set_weights(w_init[i][j])\n",
    "                \n",
    "    def printWeights(self):\n",
    "        print()\n",
    "        for i in range(1,len(self.network)):\n",
    "            for j in range(self.layers[i]):\n",
    "                print(\"Layer\",i+1,\"Neuron\",j,self.network[i][j].weights)\n",
    "            print()\n",
    "            \n",
    "    def run(self,x):\n",
    "        \"\"\"Feed a sample x into the multilayer perceptron\"\"\"\n",
    "        x=np.array(x,dtype=object)\n",
    "        self.values[0]=x\n",
    "        for i in range(1,len(self.network)):\n",
    "            for j in range(self.layers[i]):\n",
    "                self.values[i][j]= self.network[i][j].run(self.values[i-1])\n",
    "        return self.values[-1]\n",
    "        \n",
    "    #Backpropagation method\n",
    "    def bp(self, x, y):\n",
    "        \"\"\"Run a single (x,y) pair with the backpropagation algorythm).\"\"\"\n",
    "        x = np.array(x, dtype=object)\n",
    "        y = np.array(y, dtype=object)\n",
    "\n",
    "        #Step 1:  feed a sample to the network\n",
    "        outputs = self.run(x)\n",
    "\n",
    "        #Step 2: calculate the MSE\n",
    "        error = (y-outputs)\n",
    "        MSE = sum(error ** 2)/ self.layers[-1]\n",
    "\n",
    "        #Step 3: Calculate the output error terms\n",
    "        self.d[-1] = outputs * (1-outputs)*error\n",
    "\n",
    "        #Step 4: Calculate the error term of each unit on each layer\n",
    "        for i in reversed(range(1,len(self.network)-1)):\n",
    "            for h in range(len(self.network[i])):\n",
    "                fwd_error = 0.0\n",
    "                for k in range(self.layers[i+1]):\n",
    "                    fwd_error+= self.network[i+1][k].weights[h] * self.d[i+1][k]\n",
    "                self.d[i][h]=self.values[i][h] * (1-self.values[i][h]) * fwd_error\n",
    "\n",
    "        #step 5 & 6 : calculate the deltas and update the weights\n",
    "        #iterates layers\n",
    "        for i in range(1, len(self.network)):\n",
    "            #iterates neurons\n",
    "            for j in range(self.layers[i]):\n",
    "                #iterates inputs\n",
    "                for k in range(self.layers[i-1]+1):\n",
    "                    if k == self.layers[i-1]:\n",
    "                        delta = self.eta * self.d[i][j] *self.bias\n",
    "                    else:\n",
    "                        delta = self.eta * self.d[i][j] * self.values[i-1][k]\n",
    "                    self.network[i][j].weights[k]+=delta\n",
    "\n",
    "        return MSE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8347251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training neural networks as an XOR gate \n",
      "\n",
      "0.2707328017146026\n",
      "0.2645924002509357\n",
      "0.26380612695686995\n",
      "0.26306668471077627\n",
      "0.2624022437826665\n",
      "0.26181714261251426\n",
      "0.2612852670283703\n",
      "0.26076620615016255\n",
      "0.2602005402629963\n",
      "0.2594633370641857\n",
      "0.258155699922761\n",
      "0.25471139193572045\n",
      "0.2447182760239549\n",
      "0.22431141392152976\n",
      "0.20237926078221777\n",
      "0.1886262099341123\n",
      "0.1795547485146825\n",
      "0.1696066292124495\n",
      "0.14884088100319248\n",
      "0.10024895020600852\n",
      "0.049273100328090264\n",
      "0.025961455880009893\n",
      "0.01622236181129758\n",
      "0.011405770068928804\n",
      "0.008649916202985523\n",
      "0.006901571000041712\n",
      "0.005707620068594713\n",
      "0.0048467810544767795\n",
      "0.004199913882959844\n",
      "0.003697823531762134\n",
      "\n",
      "Layer 2 Neuron 0 [-3.84774915 -3.84788153  5.64692394]\n",
      "Layer 2 Neuron 1 [-5.92131072 -5.95368863  2.21772293]\n",
      "\n",
      "Layer 3 Neuron 0 [ 7.56050448 -7.85012787 -3.44721102]\n",
      "\n",
      "MLP:\n",
      "0 0 = 0.0477536623\n",
      "0 1 = 0.9456835593\n",
      "1 0 = 0.9453873895\n",
      "1 1 = 0.0701471579\n"
     ]
    }
   ],
   "source": [
    "#test code\n",
    "mlp = MultiLayerPerceptron(layers=[2,2,1])\n",
    "print(\"Training neural networks as an XOR gate \\n\")\n",
    "for i in range(3000):\n",
    "    mse=0.0\n",
    "    mse+=mlp.bp([0,0],[0])\n",
    "    mse+=mlp.bp([0,1],[1])\n",
    "    mse+=mlp.bp([1,0],[1])\n",
    "    mse+=mlp.bp([1,1],[0])\n",
    "    mse = mse/4\n",
    "    if(i%100 == 0):\n",
    "        print(mse)\n",
    "        \n",
    "mlp.printWeights()\n",
    "\n",
    "print(\"MLP:\")\n",
    "print(\"0 0 = {0:.10f}\".format(mlp.run([0,0])[0]))\n",
    "print(\"0 1 = {0:.10f}\".format(mlp.run([0,1])[0]))\n",
    "print(\"1 0 = {0:.10f}\".format(mlp.run([1,0])[0]))\n",
    "print(\"1 1 = {0:.10f}\".format(mlp.run([1,1])[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
